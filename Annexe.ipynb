{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEXE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for correlation between risk caracteristics\n",
    "claims_data[['VehPower', 'DrivAge', 'BonusMalus', 'Density', 'Exposure']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Claim_counts holds historical claim frequency data\n",
    "# Non_zero_claims holds historical non-zero claim severities\n",
    "\n",
    "# Use the fitted parameters from the Negative Binomial and Log-normal fits\n",
    "# For claim frequency (Negative Binomial)\n",
    "params_negbin = [r_negbin, p_negbin]  # Fitted Negative Binomial params\n",
    "# For claim severity (Log-normal)\n",
    "params_lognorm = [shape, loc, scale]  # Fitted Log-normal params\n",
    "\n",
    "# Number of simulations and policies\n",
    "n_simulations = 1000\n",
    "n_policies = len(claims_data)\n",
    "\n",
    "# Pre-allocate an array for total losses\n",
    "total_losses = np.zeros(n_simulations)\n",
    "\n",
    "# Monte Carlo simulation loop\n",
    "for i in range(n_simulations):\n",
    "    # Simulate number of claims for all policies at once\n",
    "    num_claims = nbinom.rvs(r_negbin, p_negbin, size=n_policies)\n",
    "    \n",
    "    # Initialize total claim severity for each policy\n",
    "    total_claim_severity = np.zeros(n_policies)\n",
    "    \n",
    "    # For policies with claims, simulate the severity\n",
    "    for j in range(n_policies):\n",
    "        if num_claims[j] > 0:\n",
    "            # Simulate claim severity for each claim and sum them for the policy\n",
    "            total_claim_severity[j] = lognorm.rvs(shape, loc, scale, size=num_claims[j]).sum()\n",
    "    \n",
    "    # Store the total loss for this simulation\n",
    "    total_losses[i] = total_claim_severity.sum()\n",
    "\n",
    "# Calculate expected total loss from the simulation results \n",
    "expected_total_loss = np.mean(total_losses)\n",
    "\n",
    "# Calculate Value-at-Risk (VaR) at different confidence levels (95% and 99%)\n",
    "var_95 = np.percentile(total_losses, 95)\n",
    "var_99 = np.percentile(total_losses, 99)\n",
    "\n",
    "# Plot the total losses distribution and highlight VaR levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(total_losses, bins=50, color='skyblue', alpha=0.7)\n",
    "plt.axvline(var_95, color='red', linestyle='dashed', linewidth=2, label=f'VaR 95%: {var_95:.2f}')\n",
    "plt.axvline(var_99, color='orange', linestyle='dashed', linewidth=2, label=f'VaR 99%: {var_99:.2f}')\n",
    "plt.title('Distribution of Total Losses from Monte Carlo Simulation')\n",
    "plt.xlabel('Total Losses')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Printing Risk Measures \n",
    "print(f\"Expected Total Loss: {expected_total_loss:.2f}\")\n",
    "print(f\"95th Percentile Total Loss (VaR 95%): {var_95:.2f}\")\n",
    "print(f\"99th Percentile Total Loss (VaR 99%): {var_99:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fix: Ensure df_1 is a correct copy of claims_data\n",
    "df_1 = claims_data.copy()\n",
    "\n",
    "# Columns to consider for clustering:\n",
    "# We will treat 'Age Group', 'Vehicle Group', and 'Vehicle power' as categorical for now.\n",
    "categorical_columns = ['Age Group', 'Vehicle Group', 'Vehicle power', 'Bonus / Malus', 'Area', 'Region']\n",
    "\n",
    "# Step 1: Data Preprocessing using ColumnTransformer\n",
    "# One-hot encode categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_columns)  # One-hot encode categorical columns\n",
    "    ])\n",
    "\n",
    "# Step 2: Build the pipeline\n",
    "# The pipeline will first preprocess the data and then apply KMeans clustering\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('kmeans', KMeans(n_clusters=3, random_state=42))])\n",
    "\n",
    "# Step 3: Fit the pipeline to your data\n",
    "pipeline.fit(df_1)\n",
    "\n",
    "# Step 4: Predict clusters and store the result in 'Risk Cluster'\n",
    "df_1['Risk Cluster'] = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "# Step 5: Analyze the clusters\n",
    "# Count how many data points belong to each cluster\n",
    "print(df_1['Risk Cluster'].value_counts())\n",
    "\n",
    "# Optional: Display the DataFrame with the new 'Risk Cluster' column\n",
    "print(df_1.head())\n",
    "\n",
    "# Step 6: Cluster summary\n",
    "# Since most columns are categorical, we will calculate frequency counts instead of mean\n",
    "cluster_summary = df_1.groupby('Risk Cluster').size()\n",
    "\n",
    "print(\"Cluster Summary (Cluster Size):\")\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Predict the probability of filing a claim (i.e., whether a claim is made or not)\n",
    "\n",
    "## Goal: Understand which characteristics increase or decrease the likelihood of a policyholder filing a claim (frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create binary variable for claims (1 if there was at least one claim, 0 otherwise)\n",
    "claims_data['Claim_binary_variable'] = claims_data['ClaimNb'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Convert categorical variables to dummy variables (e.g., Area, VehBrand, VehGas, Region)\n",
    "claims_data = pd.get_dummies(claims_data, columns=['Area', 'VehBrand', 'VehGas', 'Region'], drop_first=True, dtype = int)\n",
    "\n",
    "# Select the independent variables (Excluding IDpol and ClaimAmount)\n",
    "independent_vars = ['Exposure', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']\n",
    "\n",
    "# Adding dummy variables to independent_vars for policyholder's area, vehicle's brand, the type of gas used and for the region of the policyholder\n",
    "independent_vars += [col for col in claims_data.columns if col.startswith('Area_') or col.startswith('VehBrand_') or col.startswith('VehGas_') or col.startswith('Region_')]\n",
    "\n",
    "# Add constant term to the model\n",
    "X = sm.add_constant(claims_data[independent_vars])\n",
    "\n",
    "# Define the dependent variable\n",
    "y = claims_data['Claim_binary_variable']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataset for policyholders who made a claim\n",
    "claims_with_claims = claims_data[claims_data['ClaimNb'] > 0]\n",
    "\n",
    "# Log-transform the dependent variable (ClaimAmount)\n",
    "claims_with_claims['log_ClaimAmount'] = np.log(claims_with_claims['ClaimAmount'])\n",
    "\n",
    "# Define the independent variables (same as used for frequency)\n",
    "independent_vars_severity = ['Exposure', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']\n",
    "independent_vars_severity += [col for col in claims_with_claims.columns if col.startswith('Area_') or col.startswith('VehBrand_') or col.startswith('VehGas_') or col.startswith('Region_')]\n",
    "\n",
    "# Add constant term\n",
    "X_severity = sm.add_constant(claims_with_claims[independent_vars_severity])\n",
    "\n",
    "# Dependent variable (log-transformed ClaimAmount)\n",
    "y_severity = claims_with_claims['log_ClaimAmount']\n",
    "\n",
    "# Fit the GLM for claim severity with a log-normal distribution and log link function \n",
    "# Gaussian family with a log link for modeling the log-normal distribution of claim amounts \n",
    "glm_severity = sm.GLM(y_severity, X_severity, family=sm.families.Gaussian(sm.families.links.log())).fit()\n",
    "\n",
    "# View the summary of the severity model\n",
    "print(glm_severity.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture model for risk classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Définition des variables de caractéristiques et de la cible:\n",
    "X = claims_data[['VehAge', 'DrivAge', 'BonusMalus', 'VehPower', 'Area', 'VehGas']] # X contient les caractéristiques observables des données d'assurance.\n",
    "\n",
    "# Définition des caractéristiques catégorielles et numériques:\n",
    "categorical_features = ['Area', 'VehGas']\n",
    "numerical_features = ['VehAge', 'DrivAge', 'BonusMalus', 'VehPower']\n",
    "\n",
    "# Création d'un pipeline de prétraitement: \n",
    "# ColumnTransformer applique StandardScaler aux caractéristiques numériques et OneHotEncoder aux caractéristiques catégorielles:\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "# Les transformations sont appliquées aux données d'entrée X pour obtenir X prétraité:\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Applique un modèle de mélange gaussien avec 3 composantes, pour identifier les classes de risque latentes: \n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(X_preprocessed)\n",
    "\n",
    "# Les classes de risque latentes sont prédites et ajoutées aux données d'origine sous la colonne 'RiskClass':\n",
    "claims_data['RiskClass'] = gmm.predict(X_preprocessed)\n",
    "\n",
    "# Visualisation des classes de risque prédites:\n",
    "sns.pairplot(claims_data, hue='RiskClass', vars=['VehAge', 'DrivAge', 'BonusMalus', 'VehPower'])\n",
    "plt.show()\n",
    "\n",
    "claims_data[['VehAge', 'DrivAge', 'BonusMalus', 'VehPower', 'Area', 'VehGas', 'RiskClass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ajouter les prédictions de cluster aux données d'origine\n",
    "claims_data['RiskClass'] = gmm.predict(X_preprocessed)\n",
    "\n",
    "# Sélectionner uniquement les colonnes numériques\n",
    "numeric_columns = claims_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculer les statistiques descriptives pour chaque cluster en utilisant uniquement les colonnes numériques\n",
    "cluster_summary = claims_data.groupby('RiskClass')[numeric_columns].describe()\n",
    "\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to do a table in the report for % of characteristics in each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fréquences des caracteristiques dans chaque cluster (%Part de chaque caracteristiques)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate percentage of categorical variables in each cluster\n",
    "def calculate_cluster_percentages(df, cluster_num):\n",
    "    cluster_data = df[df['Risk Cluster'] == cluster_num]\n",
    "    percentages = {}\n",
    "    for col in categorical_columns:\n",
    "        percentages[col] = cluster_data[col].value_counts(normalize=True) * 100\n",
    "    return pd.DataFrame(percentages)\n",
    "\n",
    "# Calculate percentages for each cluster\n",
    "cluster_0_percentages = calculate_cluster_percentages(claims_data, 0)\n",
    "cluster_1_percentages = calculate_cluster_percentages(claims_data, 1)\n",
    "cluster_2_percentages = calculate_cluster_percentages(claims_data, 2)\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "combined_df = pd.concat([cluster_0_percentages, cluster_1_percentages, cluster_2_percentages], axis=1, keys=['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "\n",
    "# Replace NaN values with 0\n",
    "#combined_df = combined_df.fillna(0)\n",
    "\n",
    "combined_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
